{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7e0dad4",
   "metadata": {},
   "source": [
    "# Technical Challenge - Code Review and Deployment Pipeline Orchestration\n",
    "\n",
    "**Format:** Structured interview with whiteboarding/documentation  \n",
    "**Assessment Focus:** Problem decomposition, AI prompting strategy, system design\n",
    "\n",
    "**Please Fill in your Responses in the Response markdown boxes**\n",
    "\n",
    "---\n",
    "\n",
    "## Challenge Scenario\n",
    "\n",
    "You are tasked with creating an AI-powered system that can handle the complete lifecycle of code review and deployment pipeline management for a mid-size software company. The system needs to:\n",
    "\n",
    "**Current Pain Points:**\n",
    "- Manual code reviews take 2-3 days per PR\n",
    "- Inconsistent review quality across teams\n",
    "- Deployment failures due to missed edge cases\n",
    "- Security vulnerabilities slip through reviews\n",
    "- No standardized deployment process across projects\n",
    "- Rollback decisions are manual and slow\n",
    "\n",
    "**Business Requirements:**\n",
    "- Reduce review time to <4 hours for standard PRs\n",
    "- Maintain or improve code quality\n",
    "- Catch 90%+ of security vulnerabilities before deployment\n",
    "- Standardize deployment across 50+ microservices\n",
    "- Enable automatic rollback based on metrics\n",
    "- Support multiple environments (dev, staging, prod)\n",
    "- Handle both new features and hotfixes\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be761411",
   "metadata": {},
   "source": [
    "## Part A: Problem Decomposition (25 points)\n",
    "\n",
    "**Question 1.1:** Break this challenge down into discrete, manageable steps that could be handled by AI agents or automated systems. Each step should have:\n",
    "- Clear input requirements\n",
    "- Specific output format\n",
    "- Success criteria\n",
    "- Failure handling strategy\n",
    "\n",
    "**Question 1.2:** Which steps can run in parallel? Which are blocking? Where are the critical decision points?\n",
    "\n",
    "**Question 1.3:** Identify the key handoff points between steps. What data/context needs to be passed between each phase?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0a3c10",
   "metadata": {},
   "source": "## Response Part A: Problem Decomposition\n\n### Question 1.1: Discrete Steps with Clear Requirements\n\n#### **Step 1: PR Intake & Analysis**\n- **Input**: PR metadata (files changed, commit messages, author, base/target branch)\n- **Output**: Structured PR profile (language, scope, risk level, estimated complexity)\n- **Success Criteria**: All files parsed, dependencies identified, risk score calculated\n- **Failure Handling**: Flag for manual review if parsing fails; default to high-risk category\n\n#### **Step 2: Code Quality Analysis (Automated Linting)**\n- **Input**: Changed files, project configuration (.eslintrc, .pylintrc, etc.)\n- **Output**: Lint violations with severity (error/warning), line numbers, auto-fix suggestions\n- **Success Criteria**: All files linted, violations categorized\n- **Failure Handling**: Partial results accepted; unsupported files skipped with notification\n\n#### **Step 3: Security Vulnerability Scan**\n- **Input**: Changed files, dependency manifests (package.json, requirements.txt)\n- **Output**: CVE list, OWASP Top 10 violations, severity scores, remediation steps\n- **Success Criteria**: All security rules executed, dependencies scanned for known vulnerabilities\n- **Failure Handling**: Fail-safe to block if scanner errors; manual security review required\n\n#### **Step 4: AI-Powered Code Review (Logic & Best Practices)**\n- **Input**: Code diff, PR description, project context (README, architecture docs)\n- **Output**: Structured review comments (bugs, performance issues, maintainability, suggestions)\n- **Success Criteria**: Review completed within 10 minutes, >5 actionable comments generated\n- **Failure Handling**: Timeout triggers simplified review; retry with reduced context if LLM fails\n\n#### **Step 5: Test Coverage & Validation**\n- **Input**: Test files, code coverage report, CI test results\n- **Output**: Coverage delta, missing test scenarios, risk areas\n- **Success Criteria**: Coverage calculated, critical paths identified\n- **Failure Handling**: Warn if coverage unavailable; recommend manual testing\n\n#### **Step 6: Performance Impact Analysis**\n- **Input**: Changed files, performance benchmarks, database queries\n- **Output**: Performance risk score, query efficiency analysis, N+1 detection\n- **Success Criteria**: All queries analyzed, benchmark comparisons made\n- **Failure Handling**: Skip if benchmarks unavailable; flag for manual perf testing\n\n#### **Step 7: Review Aggregation & Prioritization**\n- **Input**: Outputs from Steps 2-6\n- **Output**: Unified review report with priority-ranked issues (blocking, recommended, optional)\n- **Success Criteria**: All issues categorized, duplicates removed, actionable summary created\n- **Failure Handling**: Partial aggregation accepted; manual review fills gaps\n\n#### **Step 8: Developer Notification & Collaboration**\n- **Input**: Aggregated review, developer contact info\n- **Output**: PR comments posted, Slack/email notifications sent\n- **Success Criteria**: All stakeholders notified, comments linked to code lines\n- **Failure Handling**: Retry notification 3x; escalate to manual if fails\n\n#### **Step 9: Approval Decision (Human-in-the-Loop)**\n- **Input**: Review report, team policy (e.g., \"2 approvals required\")\n- **Output**: Approval status, merge eligibility\n- **Success Criteria**: Policy enforced, decision logged\n- **Failure Handling**: Default to \"needs review\" if logic unclear\n\n#### **Step 10: Deployment Pipeline Orchestration**\n- **Input**: Approved PR, target environment (dev/staging/prod), deployment config\n- **Output**: Deployment job triggered, health checks scheduled\n- **Success Criteria**: Deployment initiated, metrics baseline captured\n- **Failure Handling**: Rollback on health check failure; alert on-call engineer\n\n#### **Step 11: Post-Deployment Monitoring**\n- **Input**: Deployment metadata, metrics (error rate, latency, CPU), logs\n- **Output**: Health status, anomaly alerts, rollback recommendation\n- **Success Criteria**: Metrics monitored for 1 hour, no anomalies detected\n- **Failure Handling**: Auto-rollback if error rate >5%; page on-call if critical\n\n#### **Step 12: Feedback Loop & Learning**\n- **Input**: Deployment outcome, false positive/negative flags from developers\n- **Output**: Updated AI model weights, revised review rules\n- **Success Criteria**: Feedback processed, model retrained weekly\n- **Failure Handling**: Manual review of edge cases; fallback to previous model version\n\n---\n\n### Question 1.2: Parallelization & Dependencies\n\n#### **Parallel Execution:**\n- **Steps 2, 3, 4, 5, 6** can run **in parallel** after Step 1 completes (all analyze the same PR independently)\n- **Step 11** monitoring runs in parallel with production traffic (non-blocking)\n\n#### **Blocking Dependencies:**\n```\nStep 1 (PR Intake)\n    ‚Üì\n‚îú‚îÄ Step 2 (Linting) ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îú‚îÄ Step 3 (Security) ‚îÄ‚îÄ‚îÄ‚î§\n‚îú‚îÄ Step 4 (AI Review) ‚îÄ‚îÄ‚î§‚îÄ‚Üí Step 7 (Aggregation)\n‚îú‚îÄ Step 5 (Tests) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§       ‚Üì\n‚îî‚îÄ Step 6 (Performance)‚îÄ‚îò   Step 8 (Notification)\n                                 ‚Üì\n                            Step 9 (Approval)\n                                 ‚Üì\n                            Step 10 (Deployment)\n                                 ‚Üì\n                            Step 11 (Monitoring)\n                                 ‚Üì\n                            Step 12 (Feedback Loop)\n```\n\n#### **Critical Decision Points:**\n1. **After Step 3 (Security)**: If critical CVEs found ‚Üí **block merge** (hard stop)\n2. **After Step 7 (Aggregation)**: If >10 blocking issues ‚Üí **reject PR** automatically\n3. **After Step 9 (Approval)**: Human override required for high-risk deploys\n4. **After Step 11 (Monitoring)**: Auto-rollback if error rate >5% or latency >2x baseline\n\n---\n\n### Question 1.3: Key Handoff Points & Data Context\n\n| **Handoff** | **From ‚Üí To** | **Data Passed** | **Format** |\n|-------------|---------------|-----------------|------------|\n| 1 ‚Üí 2-6 | PR Intake ‚Üí Analysis Steps | File paths, diff, metadata, project config | JSON: `{pr_id, files: [...], language, base_branch}` |\n| 2-6 ‚Üí 7 | Analysis ‚Üí Aggregation | Individual findings | JSON: `{step_name, issues: [{severity, message, line, file}]}` |\n| 7 ‚Üí 8 | Aggregation ‚Üí Notification | Unified report | Markdown + JSON: `{blocking: [...], warnings: [...], summary}` |\n| 8 ‚Üí 9 | Notification ‚Üí Approval | Review status, approver list | JSON: `{approvals_needed, current_approvals, blocking_issues}` |\n| 9 ‚Üí 10 | Approval ‚Üí Deployment | Merge commit SHA, environment | JSON: `{commit_sha, env: \"prod\", config: {...}}` |\n| 10 ‚Üí 11 | Deployment ‚Üí Monitoring | Deployment timestamp, service name, metrics baseline | JSON: `{deploy_time, service, baseline_metrics: {error_rate, latency}}` |\n| 11 ‚Üí 12 | Monitoring ‚Üí Feedback | Outcome (success/rollback), developer feedback | JSON: `{outcome, false_positives: [...], missed_issues: [...]}` |\n\n**Context Preservation Strategy:**\n- **State Store**: Redis/DynamoDB to persist PR state across steps\n- **Message Queue**: Kafka/SQS for async step communication\n- **Tracing**: OpenTelemetry for distributed tracing across steps\n- **Audit Log**: Immutable log of all decisions and handoffs for compliance"
  },
  {
   "cell_type": "markdown",
   "id": "cb38e9fa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fdc377",
   "metadata": {},
   "source": [
    "## Part B: AI Prompting Strategy (30 points)\n",
    "\n",
    "**Question 2.1:** For 2 consecutive major steps you identified, design specific AI prompts that would achieve the desired outcome. Include:\n",
    "- System role/persona definition\n",
    "- Structured input format\n",
    "- Expected output format\n",
    "- Examples of good vs bad responses\n",
    "- Error handling instructions\n",
    "\n",
    "**Question 2.2:** How would you handle the following challenging scenarios with your AI prompts:\n",
    "- **Code that uses obscure libraries or frameworks**\n",
    "- **Security reviews for code**\n",
    "- **Performance analysis of database queries**\n",
    "- **Legacy code modifications**\n",
    "\n",
    "**Question 2.3:** How would you ensure your prompts are working effectively and getting consistent results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd049f5",
   "metadata": {},
   "source": "## Response Part B: AI Prompting Strategy\n\n### Question 2.1: Prompt Design for Two Consecutive Steps\n\n---\n\n#### **PROMPT 1: Step 4 - AI-Powered Code Review**\n\n**System Role:**\n```\nYou are an expert software engineer with 15+ years of experience reviewing code across \nmultiple languages and frameworks. Your role is to provide constructive, actionable \nfeedback that improves code quality, security, and maintainability. \n\nYou follow these principles:\n- Identify bugs, edge cases, and potential runtime errors\n- Suggest performance optimizations with measurable impact\n- Ensure code follows language-specific best practices\n- Highlight security vulnerabilities (injection, auth issues, data leaks)\n- Recommend refactoring only when it significantly improves readability or performance\n- Be concise and specific - cite line numbers and provide code examples\n- Distinguish between critical issues (must fix) and suggestions (nice to have)\n```\n\n**Structured Input Format:**\n```json\n{\n  \"pr_metadata\": {\n    \"pr_id\": \"PR-12345\",\n    \"title\": \"Add user authentication endpoint\",\n    \"description\": \"Implements JWT-based auth for /api/login\",\n    \"author\": \"jane.doe\",\n    \"target_branch\": \"main\"\n  },\n  \"code_diff\": \"... [git diff output] ...\",\n  \"project_context\": {\n    \"language\": \"Python\",\n    \"framework\": \"FastAPI\",\n    \"architecture\": \"microservices\",\n    \"database\": \"PostgreSQL\"\n  },\n  \"coding_standards\": {\n    \"max_function_length\": 50,\n    \"enforce_type_hints\": true,\n    \"security_rules\": [\"no-hardcoded-secrets\", \"sql-parameterization\"]\n  }\n}\n```\n\n**Expected Output Format:**\n```json\n{\n  \"review_summary\": \"Found 1 critical security issue, 2 performance concerns, 3 style suggestions\",\n  \"issues\": [\n    {\n      \"severity\": \"critical\",\n      \"category\": \"security\",\n      \"file\": \"app/auth.py\",\n      \"line\": 45,\n      \"message\": \"Hardcoded JWT secret key exposes authentication to attacks\",\n      \"recommendation\": \"Move secret to environment variable: SECRET_KEY = os.getenv('JWT_SECRET')\",\n      \"code_snippet\": \"SECRET_KEY = 'hardcoded-secret-123'  # ‚Üê CRITICAL\"\n    },\n    {\n      \"severity\": \"high\",\n      \"category\": \"performance\",\n      \"file\": \"app/db.py\",\n      \"line\": 78,\n      \"message\": \"N+1 query detected - fetching users in loop causes 1000+ DB queries\",\n      \"recommendation\": \"Use JOIN or batch query: users = db.query(User).filter(User.id.in_(user_ids)).all()\",\n      \"code_snippet\": \"for user_id in user_ids:\\n    user = db.query(User).get(user_id)  # ‚Üê N+1\"\n    },\n    {\n      \"severity\": \"medium\",\n      \"category\": \"maintainability\",\n      \"file\": \"app/utils.py\",\n      \"line\": 23,\n      \"message\": \"Function validate_input() is 85 lines - exceeds 50-line limit\",\n      \"recommendation\": \"Split into smaller functions: validate_email(), validate_password(), validate_username()\"\n    }\n  ],\n  \"positive_feedback\": [\n    \"Excellent use of type hints throughout\",\n    \"Good error handling with custom exceptions\"\n  ],\n  \"overall_risk_score\": 7.5\n}\n```\n\n**Example Good Response:**\n```\n‚úÖ Specific: \"Line 45: Hardcoded secret key\" (not \"Security issue somewhere\")\n‚úÖ Actionable: Provides exact fix with code example\n‚úÖ Prioritized: severity: \"critical\" vs \"medium\"\n‚úÖ Constructive: Includes positive feedback\n```\n\n**Example Bad Response:**\n```\n‚ùå Vague: \"The code has some problems\"\n‚ùå No location: \"There's a security issue\" (no line number)\n‚ùå Unhelpful: \"Rewrite everything\" (not actionable)\n‚ùå No severity: All issues treated equally\n```\n\n**Error Handling Instructions:**\n```\nIF code_diff is empty:\n  RETURN {\"error\": \"No code changes detected\", \"action\": \"skip_review\"}\n\nIF language not in [Python, JavaScript, TypeScript, Go, Java, Ruby]:\n  RETURN {\"warning\": \"Unsupported language\", \"action\": \"fallback_to_generic_review\"}\n\nIF unable to parse code (syntax errors):\n  RETURN {\"issues\": [], \"warning\": \"Code parsing failed - recommend manual review\"}\n\nIF timeout (>10 minutes):\n  RETURN partial results with {\"status\": \"incomplete\", \"reviewed_files\": [...]}\n```\n\n---\n\n#### **PROMPT 2: Step 7 - Review Aggregation & Prioritization**\n\n**System Role:**\n```\nYou are a technical project manager responsible for triaging code review findings \nfrom multiple automated systems. Your goal is to consolidate, deduplicate, and \nprioritize issues so developers can focus on the most critical problems first.\n\nYou follow these rules:\n- Merge duplicate issues (same line, similar message) into one\n- Prioritize critical/high severity issues at the top\n- Group issues by file to reduce context switching\n- Flag issues that conflict with each other\n- Provide a concise executive summary for non-technical stakeholders\n- Calculate an overall \"merge readiness score\" (0-100)\n```\n\n**Structured Input Format:**\n```json\n{\n  \"pr_id\": \"PR-12345\",\n  \"findings_from_steps\": {\n    \"linting\": {\n      \"tool\": \"eslint\",\n      \"issues\": [\n        {\"file\": \"app.js\", \"line\": 23, \"severity\": \"error\", \"message\": \"Unused variable 'x'\"},\n        {\"file\": \"app.js\", \"line\": 45, \"severity\": \"warning\", \"message\": \"Console.log detected\"}\n      ]\n    },\n    \"security\": {\n      \"tool\": \"Snyk\",\n      \"issues\": [\n        {\"file\": \"auth.py\", \"line\": 45, \"severity\": \"critical\", \"cve\": \"CWE-798\", \"message\": \"Hardcoded secret\"}\n      ]\n    },\n    \"ai_review\": {\n      \"tool\": \"GPT-4\",\n      \"issues\": [\n        {\"file\": \"auth.py\", \"line\": 45, \"severity\": \"critical\", \"message\": \"Hardcoded JWT secret key\"},\n        {\"file\": \"db.py\", \"line\": 78, \"severity\": \"high\", \"message\": \"N+1 query detected\"}\n      ]\n    },\n    \"test_coverage\": {\n      \"coverage_delta\": -5.2,\n      \"missing_tests\": [\"test_login_invalid_password\"]\n    }\n  }\n}\n```\n\n**Expected Output Format:**\n```json\n{\n  \"executive_summary\": \"PR has 1 critical security issue (blocking), 1 performance issue (recommended fix), and 2 minor linting warnings. Test coverage decreased by 5.2%. Recommend addressing security issue before merge.\",\n  \"merge_readiness_score\": 45,\n  \"blocking_issues\": [\n    {\n      \"id\": \"AGG-001\",\n      \"severity\": \"critical\",\n      \"category\": \"security\",\n      \"file\": \"auth.py\",\n      \"line\": 45,\n      \"deduplicated_from\": [\"security.issues[0]\", \"ai_review.issues[0]\"],\n      \"consolidated_message\": \"Hardcoded JWT secret key (CWE-798) - must be moved to environment variable\",\n      \"sources\": [\"Snyk\", \"GPT-4\"],\n      \"recommendation\": \"BLOCK merge until fixed\"\n    }\n  ],\n  \"recommended_fixes\": [\n    {\n      \"id\": \"AGG-002\",\n      \"severity\": \"high\",\n      \"file\": \"db.py\",\n      \"line\": 78,\n      \"message\": \"N+1 query - use batch query or JOIN\",\n      \"estimated_impact\": \"Reduces DB calls from 1000+ to 1\"\n    }\n  ],\n  \"optional_improvements\": [\n    {\n      \"id\": \"AGG-003\",\n      \"severity\": \"low\",\n      \"file\": \"app.js\",\n      \"line\": 23,\n      \"message\": \"Remove unused variable 'x'\"\n    }\n  ],\n  \"test_coverage_alert\": {\n    \"status\": \"degraded\",\n    \"delta\": -5.2,\n    \"recommendation\": \"Add tests for login failure scenarios\"\n  }\n}\n```\n\n**Good vs Bad Examples:**\n\n**Good Response:**\n```\n‚úÖ Deduplication: Merged \"Hardcoded secret\" from Snyk + GPT-4 into one issue\n‚úÖ Prioritization: Critical issues listed first, blocking merge\n‚úÖ Actionable summary: \"Fix auth.py line 45 before merge\"\n‚úÖ Risk assessment: merge_readiness_score = 45 (not ready)\n```\n\n**Bad Response:**\n```\n‚ùå Duplicates: Lists same issue twice from different tools\n‚ùå No prioritization: All issues mixed together\n‚ùå Vague summary: \"Some issues found\"\n‚ùå No decision: Doesn't say if PR should be blocked\n```\n\n**Error Handling:**\n```\nIF no issues found in any step:\n  RETURN {\"status\": \"clean\", \"merge_readiness_score\": 100, \"recommendation\": \"Approve\"}\n\nIF findings_from_steps is empty:\n  RETURN {\"error\": \"No analysis results available\", \"action\": \"manual_review_required\"}\n\nIF conflicting recommendations (e.g., security says block, AI says approve):\n  RETURN {\"conflict_detected\": true, \"escalate_to\": \"senior_engineer\"}\n```\n\n---\n\n### Question 2.2: Handling Challenging Scenarios\n\n#### **Scenario 1: Code Using Obscure Libraries/Frameworks**\n\n**Prompt Enhancement:**\n```\nCONTEXT: You may encounter code using unfamiliar libraries or frameworks.\n\nINSTRUCTIONS:\n1. If you recognize the library, provide specific advice (e.g., \"React Hook useEffect dependency array is incorrect\")\n2. If you don't recognize the library:\n   a. Analyze the code's *intent* based on function names, patterns, and comments\n   b. Provide general software engineering advice (error handling, validation, etc.)\n   c. Flag for human review: \"Unknown library 'obscure-lib' - recommend domain expert review\"\n3. DO NOT hallucinate library-specific advice if unsure\n4. Search project documentation (README, docs/) for library usage patterns\n\nEXAMPLE:\nInput: Code using \"FastHTML\" (obscure Python framework)\nOutput: \n- \"Unknown framework 'FastHTML' detected\"\n- \"General observation: Function fetch_data() lacks error handling for HTTP failures\"\n- \"Recommendation: Add try/except for requests.exceptions.RequestException\"\n- \"FLAG: Requires review by engineer familiar with FastHTML\"\n```\n\n---\n\n#### **Scenario 2: Security Reviews for Code**\n\n**Prompt Enhancement:**\n```\nSECURITY REVIEW MODE:\n\nFocus areas (OWASP Top 10 + CWE):\n1. **Injection (SQL, NoSQL, Command)**: Check for parameterized queries, input sanitization\n2. **Authentication**: Verify password hashing (bcrypt/Argon2), session management, MFA\n3. **Sensitive Data Exposure**: No secrets in code, encryption at rest/transit, proper access controls\n4. **XML External Entities (XXE)**: Disable external entity processing in parsers\n5. **Broken Access Control**: Verify authorization checks before data access\n6. **Security Misconfiguration**: Check default credentials, debug mode off, CORS policies\n7. **XSS**: Validate output encoding, CSP headers\n8. **Insecure Deserialization**: Avoid pickle, eval, exec with untrusted data\n9. **Using Components with Known Vulnerabilities**: Check dependency versions against CVE databases\n10. **Insufficient Logging**: Ensure security events (auth failures, access denials) are logged\n\nOUTPUT REQUIREMENTS:\n- Map findings to CWE IDs: \"CWE-89: SQL Injection on line 45\"\n- Provide exploit scenarios: \"Attacker can inject '1 OR 1=1' to bypass authentication\"\n- Severity based on CVSS: Critical (9-10), High (7-8.9), Medium (4-6.9), Low (0-3.9)\n- Include remediation code examples\n\nEXAMPLE:\nInput: `query = f\"SELECT * FROM users WHERE id = {user_id}\"`\nOutput:\n- Severity: CRITICAL\n- CWE: CWE-89 (SQL Injection)\n- Exploit: \"user_id = '1 OR 1=1; DROP TABLE users--' bypasses auth and deletes data\"\n- Fix: `query = \"SELECT * FROM users WHERE id = ?\"; cursor.execute(query, (user_id,))`\n```\n\n---\n\n#### **Scenario 3: Performance Analysis of Database Queries**\n\n**Prompt Enhancement:**\n```\nPERFORMANCE ANALYSIS MODE:\n\nDatabase query anti-patterns to detect:\n1. **N+1 Queries**: Loop with individual SELECT statements\n2. **Missing Indexes**: WHERE/JOIN on unindexed columns\n3. **SELECT ***: Fetching unnecessary columns\n4. **Unbounded Queries**: No LIMIT on result sets\n5. **Implicit Type Conversion**: Mismatched column types in WHERE clauses\n6. **Subquery in SELECT**: Can often be optimized with JOIN\n7. **Lack of Query Caching**: Repeated identical queries\n\nANALYSIS STEPS:\n1. Extract all SQL queries from code changes\n2. Identify query patterns (loop ‚Üí N+1, missing LIMIT ‚Üí unbounded)\n3. Estimate impact: queries_per_request * avg_execution_time\n4. Provide optimized alternatives with EXPLAIN PLAN if possible\n\nOUTPUT FORMAT:\n{\n  \"query\": \"SELECT * FROM users WHERE email = ?\",\n  \"issue\": \"SELECT * fetches all 50 columns when only email, name needed\",\n  \"impact\": \"Increases network transfer by 400%, slows query by 2x\",\n  \"optimized_query\": \"SELECT email, name FROM users WHERE email = ?\",\n  \"add_index\": \"CREATE INDEX idx_users_email ON users(email)\"\n}\n\nEDGE CASE: If query uses ORM (SQLAlchemy, Hibernate):\n- Analyze ORM patterns: eager loading, lazy loading, N+1 detection\n- Suggest ORM optimizations: joinedload(), selectinload()\n```\n\n---\n\n#### **Scenario 4: Legacy Code Modifications**\n\n**Prompt Enhancement:**\n```\nLEGACY CODE REVIEW MODE:\n\nContext: Code may be old (5-10+ years), lack tests, use deprecated patterns.\n\nADJUSTED EXPECTATIONS:\n- **Tolerate older patterns**: Don't demand full refactor (risk of breaking changes)\n- **Focus on safety**: Ensure new code doesn't break existing functionality\n- **Test requirements**: Require tests for new code, even if legacy lacks them\n- **Incremental improvement**: Suggest small, safe refactorings alongside new features\n\nREVIEW PRIORITIES (in order):\n1. Does new code introduce security vulnerabilities?\n2. Does new code break backward compatibility?\n3. Is new code tested (even if legacy isn't)?\n4. Does new code follow current best practices (without requiring legacy refactor)?\n\nOUTPUT GUIDELINES:\n- \"New code adds SQL injection risk - fix required\"  ‚úÖ\n- \"Legacy code lacks error handling - acceptable, but new code should include it\"  ‚úÖ\n- \"Entire file should be rewritten\"  ‚ùå (too risky for legacy)\n\nEXAMPLE:\nInput: Adding a new API endpoint to a 10-year-old Express app (uses callbacks, no async/await)\nOutput:\n- \"Legacy code uses callbacks - acceptable for existing routes\"\n- \"NEW code in routes/new-endpoint.js should use async/await (modern best practice)\"\n- \"Recommend adding tests for new endpoint (even though legacy routes lack them)\"\n- \"Do NOT suggest rewriting all legacy routes to async/await (high risk, out of scope)\"\n```\n\n---\n\n### Question 2.3: Ensuring Prompt Effectiveness & Consistency\n\n#### **Strategy 1: Automated Prompt Testing**\n```python\n# Create test suite of PRs with known issues\ntest_cases = [\n    {\n        \"pr\": \"test-pr-sql-injection.diff\",\n        \"expected_issues\": [\n            {\"severity\": \"critical\", \"category\": \"security\", \"line\": 45, \"cwe\": \"CWE-89\"}\n        ]\n    },\n    {\n        \"pr\": \"test-pr-n+1-query.diff\",\n        \"expected_issues\": [\n            {\"severity\": \"high\", \"category\": \"performance\", \"pattern\": \"N+1\"}\n        ]\n    }\n]\n\n# Run AI review on test cases weekly\nfor test in test_cases:\n    result = ai_review(test[\"pr\"])\n    assert all(expected in result[\"issues\"] for expected in test[\"expected_issues\"])\n```\n\n**Metrics to Track:**\n- **Precision**: % of AI-flagged issues that are true positives (target: >80%)\n- **Recall**: % of actual issues detected by AI (target: >90% for critical issues)\n- **Consistency**: % of identical PRs receiving same review (target: >95%)\n- **False Positive Rate**: % of flagged issues developers mark as \"not an issue\" (target: <10%)\n\n---\n\n#### **Strategy 2: Human Feedback Loop**\n```json\n// After each review, collect developer feedback\n{\n  \"pr_id\": \"PR-12345\",\n  \"ai_review_id\": \"REV-98765\",\n  \"developer_feedback\": {\n    \"issue_AGG-001\": {\"accurate\": true, \"helpful\": true},\n    \"issue_AGG-002\": {\"accurate\": false, \"reason\": \"This is intentional for legacy compatibility\"},\n    \"issue_AGG-003\": {\"accurate\": true, \"helpful\": false, \"reason\": \"Too minor, wasted time\"}\n  }\n}\n```\n\n**Use feedback to:**\n1. Retrain AI model with false positives/negatives\n2. Adjust severity thresholds (e.g., if devs ignore \"medium\" issues, elevate important ones to \"high\")\n3. Refine prompts (e.g., if AI misses N+1 queries, add more examples to prompt)\n\n---\n\n#### **Strategy 3: A/B Testing Prompts**\n- **Run two prompt versions** on same PRs for 2 weeks\n- Compare: precision, recall, developer satisfaction scores\n- Example: Test \"strict security mode\" vs \"balanced mode\" prompts\n- Gradually roll out winning prompt to 100% of reviews\n\n---\n\n#### **Strategy 4: Prompt Version Control**\n```yaml\n# prompts/code-review-v2.3.yaml\nversion: \"2.3\"\nupdated: \"2025-01-15\"\nchanges:\n  - \"Added CWE mapping for security issues\"\n  - \"Reduced false positives for legacy code (exclude callback patterns)\"\ntests_passing: 45/50\nprecision: 85%\nrecall: 92%\nrollout_status: \"production\"\n```\n\n**Maintain prompt history:**\n- Git version control for all prompts\n- Rollback capability if new prompt performs worse\n- Changelog documenting why each change was made"
  },
  {
   "cell_type": "markdown",
   "id": "476e98d3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59d353d",
   "metadata": {},
   "source": [
    "## Part C: System Architecture & Reusability (25 points)\n",
    "\n",
    "**Question 3.1:** How would you make this system reusable across different projects/teams? Consider:\n",
    "- Configuration management\n",
    "- Language/framework variations\n",
    "- Different deployment targets (cloud providers, on-prem)\n",
    "- Team-specific coding standards\n",
    "- Industry-specific compliance requirements\n",
    "\n",
    "**Question 3.2:** How would the system get better over time based on:\n",
    "- False positive/negative rates in reviews\n",
    "- Deployment success/failure patterns\n",
    "- Developer feedback\n",
    "- Production incident correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0052f045",
   "metadata": {},
   "source": "## Response Part C: System Architecture & Reusability\n\n### Question 3.1: Making the System Reusable Across Projects/Teams\n\n#### **1. Configuration Management**\n\n**Hierarchical Configuration Model:**\n```yaml\n# config/global-defaults.yaml\nreview_pipeline:\n  timeout: 600  # 10 minutes\n  parallel_steps: true\n  required_checks: [\"security\", \"linting\", \"ai_review\"]\n  \ndeployment:\n  approval_policy: \"two_approvals\"\n  auto_rollback_threshold:\n    error_rate_increase: 0.05  # 5%\n    latency_increase: 2.0      # 2x\n```\n\n```yaml\n# config/team-overrides/backend-team.yaml\nreview_pipeline:\n  timeout: 900  # Backend needs more time for complex reviews\n  coding_standards:\n    language: \"Python\"\n    max_function_length: 50\n    enforce_type_hints: true\n  security_rules:\n    - \"no-hardcoded-secrets\"\n    - \"sql-parameterization\"\n    - \"require-auth-decorators\"\n```\n\n```yaml\n# config/project-overrides/payment-service.yaml\n# Inherits from backend-team + global-defaults\ndeployment:\n  approval_policy: \"three_approvals\"  # Higher risk = more approvals\n  environments:\n    - name: \"dev\"\n      auto_deploy: true\n    - name: \"staging\"\n      auto_deploy: true\n      smoke_tests_required: true\n    - name: \"prod\"\n      auto_deploy: false  # Manual trigger for payment service\n      monitoring_duration: 7200  # 2 hours\n```\n\n**Configuration Resolution Order:**\n1. **Global defaults** (applies to all)\n2. **Team overrides** (applies to team's projects)\n3. **Project overrides** (highest priority)\n\n---\n\n#### **2. Language/Framework Variations**\n\n**Plugin Architecture for Language Support:**\n\n```python\n# plugins/base.py\nclass LanguagePlugin(ABC):\n    @abstractmethod\n    def lint(self, files: List[str]) -> LintResults:\n        pass\n    \n    @abstractmethod\n    def extract_dependencies(self, manifest_file: str) -> List[Dependency]:\n        pass\n    \n    @abstractmethod\n    def run_tests(self, test_command: str) -> TestResults:\n        pass\n\n# plugins/python_plugin.py\nclass PythonPlugin(LanguagePlugin):\n    def lint(self, files):\n        return run_tool(\"pylint\", files) + run_tool(\"mypy\", files)\n    \n    def extract_dependencies(self, manifest_file):\n        # Parse requirements.txt or pyproject.toml\n        return parse_python_dependencies(manifest_file)\n    \n    def run_tests(self, test_command):\n        return subprocess.run([\"pytest\", \"--cov\", \"--json-report\"])\n\n# plugins/javascript_plugin.py\nclass JavaScriptPlugin(LanguagePlugin):\n    def lint(self, files):\n        return run_tool(\"eslint\", files) + run_tool(\"tsc\", \"--noEmit\")\n    \n    def extract_dependencies(self, manifest_file):\n        # Parse package.json\n        return parse_npm_dependencies(manifest_file)\n    \n    def run_tests(self, test_command):\n        return subprocess.run([\"npm\", \"test\", \"--\", \"--coverage\", \"--json\"])\n```\n\n**Auto-Detection:**\n```python\ndef detect_language(pr_files):\n    file_extensions = {f.split(\".\")[-1] for f in pr_files}\n    \n    if \"py\" in file_extensions:\n        return \"python\"\n    elif \"js\" in file_extensions or \"ts\" in file_extensions:\n        return \"javascript\"\n    elif \"go\" in file_extensions:\n        return \"go\"\n    # ... etc\n    \n    # Fallback: check for manifest files\n    if \"package.json\" in pr_files:\n        return \"javascript\"\n    elif \"requirements.txt\" in pr_files or \"pyproject.toml\" in pr_files:\n        return \"python\"\n```\n\n---\n\n#### **3. Different Deployment Targets (Cloud Providers, On-Prem)**\n\n**Deployment Adapter Pattern:**\n\n```python\n# deployers/base.py\nclass DeploymentTarget(ABC):\n    @abstractmethod\n    def deploy(self, artifact: Artifact, environment: str) -> DeploymentResult:\n        pass\n    \n    @abstractmethod\n    def rollback(self, deployment_id: str) -> RollbackResult:\n        pass\n    \n    @abstractmethod\n    def get_metrics(self, service_name: str) -> Metrics:\n        pass\n\n# deployers/aws.py\nclass AWSDeployer(DeploymentTarget):\n    def deploy(self, artifact, environment):\n        # Deploy to ECS/EKS/Lambda based on service type\n        if artifact.type == \"container\":\n            return self.deploy_to_ecs(artifact, environment)\n        elif artifact.type == \"lambda\":\n            return self.deploy_lambda(artifact, environment)\n    \n    def get_metrics(self, service_name):\n        # Fetch from CloudWatch\n        return cloudwatch.get_metrics(service_name, metrics=[\"error_rate\", \"latency\", \"cpu\"])\n\n# deployers/kubernetes.py\nclass KubernetesDeployer(DeploymentTarget):\n    def deploy(self, artifact, environment):\n        # Apply Kubernetes manifests via kubectl or Helm\n        return kubectl.apply(artifact.manifest, namespace=environment)\n    \n    def get_metrics(self, service_name):\n        # Fetch from Prometheus\n        return prometheus.query(f'rate(http_requests_total{{service=\"{service_name}\"}}[5m])')\n\n# deployers/on_prem.py\nclass OnPremDeployer(DeploymentTarget):\n    def deploy(self, artifact, environment):\n        # SSH to servers, rsync files, restart services\n        for server in self.get_servers(environment):\n            ssh.upload(artifact.path, server, \"/opt/app\")\n            ssh.run(server, \"systemctl restart app\")\n```\n\n**Configuration:**\n```yaml\n# project config\ndeployment_target: \"aws\"  # or \"kubernetes\", \"on_prem\", \"azure\", \"gcp\"\ndeployment_config:\n  aws:\n    region: \"us-east-1\"\n    ecs_cluster: \"prod-cluster\"\n    task_definition: \"my-service:latest\"\n  kubernetes:\n    context: \"prod-cluster\"\n    namespace: \"production\"\n    helm_chart: \"charts/my-service\"\n```\n\n---\n\n#### **4. Team-Specific Coding Standards**\n\n**Customizable Rule Engine:**\n\n```python\n# Coding standard rules defined as plugins\nclass CodingStandardRule(ABC):\n    @abstractmethod\n    def check(self, code: str, context: Dict) -> List[Violation]:\n        pass\n\n# team_standards/backend_team.py\nclass NoGlobalVariablesRule(CodingStandardRule):\n    def check(self, code, context):\n        violations = []\n        ast_tree = ast.parse(code)\n        for node in ast.walk(ast_tree):\n            if isinstance(node, ast.Global):\n                violations.append(Violation(\n                    line=node.lineno,\n                    message=\"Global variables prohibited (team standard)\",\n                    severity=\"medium\"\n                ))\n        return violations\n\nclass RequireDocsringsRule(CodingStandardRule):\n    def check(self, code, context):\n        # Check that all public functions have docstrings\n        # ...\n```\n\n**Team Configuration:**\n```yaml\n# config/team-standards/backend-team.yaml\ncoding_standards:\n  rules:\n    - name: \"NoGlobalVariables\"\n      enabled: true\n    - name: \"RequireDocstrings\"\n      enabled: true\n      config:\n        min_function_length: 10  # Only enforce for functions >10 lines\n    - name: \"MaxComplexity\"\n      enabled: true\n      config:\n        max_cyclomatic_complexity: 15\n```\n\n---\n\n#### **5. Industry-Specific Compliance Requirements**\n\n**Compliance Framework System:**\n\n```python\n# compliance/frameworks.py\nclass ComplianceFramework(ABC):\n    @abstractmethod\n    def get_required_checks(self) -> List[ComplianceCheck]:\n        pass\n\n# compliance/pci_dss.py\nclass PCIDSS(ComplianceFramework):\n    def get_required_checks(self):\n        return [\n            SecurityCheck(\"no-hardcoded-secrets\", severity=\"critical\"),\n            SecurityCheck(\"encrypt-sensitive-data\", severity=\"critical\"),\n            SecurityCheck(\"log-access-to-cardholder-data\", severity=\"high\"),\n            DeploymentCheck(\"require-change-approval\", severity=\"high\"),\n            DeploymentCheck(\"automated-security-scan\", severity=\"high\"),\n        ]\n\n# compliance/hipaa.py\nclass HIPAA(ComplianceFramework):\n    def get_required_checks(self):\n        return [\n            SecurityCheck(\"encrypt-phi-at-rest\", severity=\"critical\"),\n            SecurityCheck(\"encrypt-phi-in-transit\", severity=\"critical\"),\n            SecurityCheck(\"access-control-for-phi\", severity=\"critical\"),\n            AuditCheck(\"log-phi-access\", severity=\"high\"),\n            DeploymentCheck(\"require-security-review\", severity=\"high\"),\n        ]\n```\n\n**Project Configuration:**\n```yaml\n# config/project/payment-service.yaml\ncompliance_frameworks:\n  - \"PCI-DSS\"  # Credit card processing\n  \n# config/project/patient-portal.yaml\ncompliance_frameworks:\n  - \"HIPAA\"  # Healthcare data\n  - \"SOC2\"   # Security controls\n```\n\n**Enforcement:**\n```python\ndef enforce_compliance(pr, project_config):\n    frameworks = [load_framework(f) for f in project_config.compliance_frameworks]\n    required_checks = []\n    \n    for framework in frameworks:\n        required_checks.extend(framework.get_required_checks())\n    \n    # Run all required checks\n    results = run_checks(pr, required_checks)\n    \n    # BLOCK merge if any critical compliance check fails\n    critical_failures = [r for r in results if r.severity == \"critical\" and r.status == \"failed\"]\n    if critical_failures:\n        return ReviewDecision(\n            status=\"blocked\",\n            reason=f\"Compliance violations: {', '.join([f.name for f in critical_failures])}\"\n        )\n```\n\n---\n\n### Question 3.2: Continuous Improvement & Learning\n\n#### **1. Reducing False Positive/Negative Rates**\n\n**Feedback Collection:**\n```python\n# After each review, prompt developer for feedback\n@post_review\ndef collect_feedback(review_id, pr_id):\n    # Add interactive buttons to PR comment\n    github.add_comment(pr_id, f\"\"\"\n    ## AI Review Complete\n    \n    Found 5 issues. Were these findings helpful?\n    \n    - Issue #1: SQL injection on line 45\n      üëç Accurate | üëé False Positive | ü§∑ Not Sure\n    \n    - Issue #2: N+1 query on line 78\n      üëç Accurate | üëé False Positive | ü§∑ Not Sure\n    \"\"\")\n\n# Store feedback\nclass ReviewFeedback:\n    review_id: str\n    issue_id: str\n    developer_rating: Literal[\"accurate\", \"false_positive\", \"false_negative\", \"not_helpful\"]\n    developer_comment: Optional[str]\n    timestamp: datetime\n```\n\n**Model Retraining:**\n```python\ndef retrain_model_weekly():\n    # Collect feedback from past week\n    feedback = db.query(ReviewFeedback).filter(\n        ReviewFeedback.timestamp > datetime.now() - timedelta(days=7)\n    )\n    \n    # Identify patterns in false positives\n    false_positives = [f for f in feedback if f.developer_rating == \"false_positive\"]\n    \n    # Example: If AI consistently flags \"legacy callback patterns\" as issues\n    # but developers mark them as false positives, add exclusion rule\n    if count_pattern(false_positives, \"legacy callback\") > 10:\n        add_exclusion_rule(\"ignore callback patterns in files matching **/legacy/**\")\n    \n    # Fine-tune LLM with feedback examples\n    training_data = [\n        {\"code\": f.code, \"expected_issues\": f.actual_issues, \"model_output\": f.ai_issues}\n        for f in feedback\n    ]\n    fine_tune_model(training_data)\n```\n\n---\n\n#### **2. Learning from Deployment Success/Failure Patterns**\n\n**Deployment Outcome Tracking:**\n```python\nclass DeploymentOutcome:\n    deployment_id: str\n    pr_id: str\n    environment: str\n    status: Literal[\"success\", \"rollback\", \"partial_failure\"]\n    issues_detected: List[str]  # e.g., [\"error_rate_spike\", \"memory_leak\"]\n    review_warnings_ignored: List[str]  # Issues AI flagged but were ignored\n    \n# Track correlation between ignored warnings and deployment failures\ndef analyze_deployment_failures():\n    failed_deployments = db.query(DeploymentOutcome).filter(\n        DeploymentOutcome.status.in_([\"rollback\", \"partial_failure\"])\n    )\n    \n    correlation = {}\n    for deployment in failed_deployments:\n        for warning in deployment.review_warnings_ignored:\n            correlation[warning.category] = correlation.get(warning.category, 0) + 1\n    \n    # Example output: {\"performance\": 15, \"null_pointer\": 8, \"thread_safety\": 3}\n    # ‚Üí Increase severity of \"performance\" warnings (they often cause prod issues)\n    \n    if correlation.get(\"performance\", 0) > 10:\n        update_rule_severity(\"performance_warnings\", from_=\"medium\", to=\"high\")\n```\n\n**Predictive Deployment Risk Scoring:**\n```python\ndef calculate_deployment_risk(pr):\n    risk_score = 0\n    \n    # Historical pattern: PRs with >5 unresolved warnings have 40% rollback rate\n    if len(pr.unresolved_warnings) > 5:\n        risk_score += 40\n    \n    # Historical pattern: Changes to auth code have 25% higher failure rate\n    if any(\"auth\" in file.path for file in pr.changed_files):\n        risk_score += 25\n    \n    # Historical pattern: Friday deploys have 30% higher rollback rate\n    if datetime.now().weekday() == 4:  # Friday\n        risk_score += 30\n    \n    # Recommend additional testing if risk > 50\n    if risk_score > 50:\n        return DeploymentRecommendation(\n            risk_level=\"high\",\n            actions=[\"Run full regression suite\", \"Deploy to staging for 24hrs\", \"Schedule on-call engineer\"]\n        )\n```\n\n---\n\n#### **3. Learning from Developer Feedback**\n\n**Feedback Analysis Dashboard:**\n```python\n# Weekly metrics\nmetrics = {\n    \"review_accuracy\": {\n        \"precision\": 0.85,  # 85% of flagged issues were real\n        \"recall\": 0.92,     # 92% of real issues were detected\n        \"f1_score\": 0.88\n    },\n    \"developer_satisfaction\": {\n        \"helpful_rate\": 0.78,  # 78% of reviews marked as helpful\n        \"avg_time_saved\": \"90 minutes\",\n        \"false_positive_rate\": 0.15\n    },\n    \"top_false_positive_categories\": [\n        {\"category\": \"legacy_code_patterns\", \"count\": 45},\n        {\"category\": \"intentional_performance_tradeoff\", \"count\": 23}\n    ]\n}\n\n# Auto-adjust prompts based on feedback\nif metrics[\"false_positive_rate\"] > 0.20:\n    send_alert(\"High false positive rate - review prompt configuration\")\n    \nif metrics[\"top_false_positive_categories\"][0][\"count\"] > 30:\n    category = metrics[\"top_false_positive_categories\"][0][\"category\"]\n    add_prompt_instruction(f\"Reduce sensitivity for {category}\")\n```\n\n---\n\n#### **4. Production Incident Correlation**\n\n**Link Reviews to Incidents:**\n```python\nclass ProductionIncident:\n    incident_id: str\n    service: str\n    timestamp: datetime\n    root_cause: str\n    related_pr_id: Optional[str]  # PR that introduced the bug\n    \n# Find incidents caused by code merged despite warnings\ndef correlate_incidents_with_reviews():\n    incidents = db.query(ProductionIncident).filter(\n        ProductionIncident.timestamp > datetime.now() - timedelta(days=30)\n    )\n    \n    for incident in incidents:\n        if incident.related_pr_id:\n            review = db.query(Review).filter(Review.pr_id == incident.related_pr_id).first()\n            \n            # Check if AI review flagged the root cause\n            for warning in review.warnings:\n                if warning.category == incident.root_cause:\n                    # AI caught it but was ignored - increase severity for future\n                    log_missed_catch(warning.category, incident.severity)\n\n# Adjust blocking rules based on incidents\nincident_patterns = analyze_incident_patterns()\n# Example: \"Memory leaks caused 5 incidents this month, all flagged as 'medium' warnings\"\n# ‚Üí Upgrade memory leak warnings to 'high' (blocking)\n```\n\n**Continuous Learning Loop:**\n```\nDeveloper Feedback ‚Üí False Positive Reduction\n        ‚Üì\nDeployment Outcomes ‚Üí Risk Scoring Improvement\n        ‚Üì\nProduction Incidents ‚Üí Severity Calibration\n        ‚Üì\nWeekly Retraining ‚Üí Better Reviews ‚Üí [Loop back]\n```"
  },
  {
   "cell_type": "markdown",
   "id": "6029f169",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d096eb",
   "metadata": {},
   "source": [
    "## Part D: Implementation Strategy (20 points)\n",
    "\n",
    "**Question 4.1:** Prioritize your implementation. What would you build first? Create a 6-month roadmap with:\n",
    "- MVP definition (what's the minimum viable system?)\n",
    "- Pilot program strategy\n",
    "- Rollout phases\n",
    "- Success metrics for each phase\n",
    "\n",
    "**Question 4.2:** Risk mitigation. What could go wrong and how would you handle:\n",
    "- AI making incorrect review decisions\n",
    "- System downtime during critical deployments\n",
    "- Integration failures with existing tools\n",
    "- Resistance from development teams\n",
    "- Compliance/audit requirements\n",
    "\n",
    "**Question 4.3:** Tool selection. What existing tools/platforms would you integrate with or build upon:\n",
    "- Code review platforms (GitHub, GitLab, Bitbucket)\n",
    "- CI/CD systems (Jenkins, GitHub Actions, GitLab CI)\n",
    "- Monitoring tools (Datadog, New Relic, Prometheus)\n",
    "- Security scanning tools (SonarQube, Snyk, Veracode)\n",
    "- Communication tools (Slack, Teams, Jira)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fa9820",
   "metadata": {},
   "source": "## Response Part D: Implementation Strategy\n\n### Question 4.1: 6-Month Implementation Roadmap\n\n---\n\n#### **MVP Definition (Month 1-2): Minimum Viable System**\n\n**Scope**: Core code review automation for 1 pilot team\n\n**Features Included:**\n- ‚úÖ PR intake and metadata extraction\n- ‚úÖ Automated linting (ESLint/Pylint)\n- ‚úÖ Basic security scanning (Snyk/Bandit)\n- ‚úÖ Simple AI code review (GPT-4 with basic prompts)\n- ‚úÖ Review aggregation and PR commenting\n- ‚úÖ Manual approval workflow (no auto-deploy yet)\n- ‚úÖ Slack notifications\n\n**Features Excluded (for later phases):**\n- ‚ùå Deployment automation\n- ‚ùå Advanced AI prompts\n- ‚ùå Multi-language support (start with Python only)\n- ‚ùå Performance analysis\n- ‚ùå Compliance frameworks\n\n**Success Metrics for MVP:**\n- Review time < 2 hours for 80% of PRs (vs 2-3 days baseline)\n- Developer satisfaction ‚â• 70% (\"helpful\" rating)\n- False positive rate < 25%\n- Zero critical security issues slip through\n\n**Deliverables:**\n- Working system for Python projects\n- Integration with GitHub\n- Basic dashboard showing review metrics\n- Documentation for developers\n\n---\n\n#### **Phase 1 (Month 1-2): MVP Development & Pilot**\n\n| **Week** | **Activities** | **Deliverables** |\n|----------|---------------|------------------|\n| **Week 1-2** | - Set up infrastructure (GitHub App, AWS/cloud resources)<br>- Implement PR webhook listener<br>- Build linting integration (Pylint) | - GitHub App registered<br>- Webhook receiver deployed<br>- Linting step functional |\n| **Week 3-4** | - Integrate security scanner (Bandit for Python)<br>- Build basic AI review prompt<br>- Implement review aggregation logic | - Security scan working<br>- AI review returning comments<br>- Aggregated report generated |\n| **Week 5-6** | - Add PR commenting (post results to GitHub)<br>- Build Slack notifications<br>- Create simple metrics dashboard | - Comments appear on PRs<br>- Slack alerts working<br>- Dashboard shows review count, avg time |\n| **Week 7-8** | - **Pilot launch** with 1 backend team (5-10 engineers)<br>- Daily standups to gather feedback<br>- Bug fixes and UX improvements | - System processing real PRs<br>- Feedback collected<br>- Initial metrics report |\n\n**Pilot Team Selection Criteria:**\n- Willing to experiment and provide feedback\n- Python-based projects (MVP language)\n- Not working on critical production features (reduce risk)\n- ~5-10 engineers (manageable scale)\n\n**Success Criteria for Pilot:**\n- Process ‚â•50 PRs successfully\n- ‚â•70% developer satisfaction\n- Identify and fix ‚â•10 bugs/UX issues\n- Measure baseline metrics (review time, issue detection rate)\n\n---\n\n#### **Phase 2 (Month 3-4): Expand & Enhance**\n\n**Goals**: Add deployment automation, support more languages, refine AI prompts\n\n| **Week** | **Activities** | **Deliverables** |\n|----------|---------------|------------------|\n| **Week 9-10** | - Add JavaScript/TypeScript support (ESLint, npm audit)<br>- Refine AI prompts based on pilot feedback<br>- Implement test coverage analysis | - JS/TS projects supported<br>- AI prompt v2 deployed<br>- Coverage reports in reviews |\n| **Week 11-12** | - Build deployment pipeline orchestration<br>- Integrate with CI/CD (GitHub Actions)<br>- Implement dev environment auto-deploy | - Deployment to dev automated<br>- CI/CD integration working<br>- Deploy button in PR UI |\n| **Week 13-14** | - Add post-deployment monitoring (basic health checks)<br>- Implement simple rollback automation<br>- Expand to 2 more teams (frontend, DevOps) | - Monitoring step functional<br>- Auto-rollback on errors<br>- 3 teams using system |\n| **Week 15-16** | - Performance testing and optimization<br>- Dashboard v2 (deployment metrics, rollback tracking)<br>- Documentation updates | - System handles 100+ PRs/week<br>- Dashboard shows deploy success rate<br>- Runbooks updated |\n\n**Success Metrics for Phase 2:**\n- Review time < 1 hour for 80% of PRs\n- Deployment success rate ‚â• 95%\n- Rollback rate < 5%\n- Support 3 teams, 20-30 engineers\n\n---\n\n#### **Phase 3 (Month 5-6): Scale & Compliance**\n\n**Goals**: Company-wide rollout, add compliance frameworks, advanced features\n\n| **Week** | **Activities** | **Deliverables** |\n|----------|---------------|------------------|\n| **Week 17-18** | - Add compliance frameworks (PCI-DSS, SOC 2)<br>- Implement performance analysis (N+1 query detection)<br>- Add Go and Java language support | - Compliance checks active<br>- Performance warnings generated<br>- 4 languages supported |\n| **Week 19-20** | - Build feedback loop system (thumbs up/down on reviews)<br>- Implement weekly model retraining<br>- Add advanced deployment strategies (canary, blue-green) | - Feedback collection working<br>- Model improves weekly<br>- Canary deploys available |\n| **Week 21-22** | - **Company-wide rollout** to all 50 teams<br>- Training sessions for engineering teams<br>- On-call rotation for support | - All teams onboarded<br>- Training materials published<br>- Support process defined |\n| **Week 23-24** | - Monitoring and optimization at scale<br>- Cost analysis and optimization<br>- Retrospective and roadmap planning | - System handles 500+ PRs/week<br>- Cost optimized<br>- 6-month retrospective report |\n\n**Success Metrics for Phase 3:**\n- Review time < 4 hours for 90% of PRs (business goal achieved)\n- Security detection rate ‚â• 90%\n- Deployment success rate ‚â• 97%\n- Developer satisfaction ‚â• 80%\n- Cost < $10,000/month\n\n---\n\n### **Rollout Strategy: Gradual Expansion**\n\n```\nMonth 1-2:  1 team (Python)          ‚Üí  10 engineers\nMonth 3-4:  3 teams (Python, JS)     ‚Üí  30 engineers  \nMonth 5-6:  50 teams (all languages) ‚Üí  500 engineers\n```\n\n**Risk Mitigation**: Gradual rollout allows us to:\n- Fix bugs at small scale before company-wide impact\n- Refine prompts based on real usage\n- Build confidence with early success stories\n- Adjust infrastructure for scale\n\n---\n\n### Question 4.2: Risk Mitigation Strategies\n\n#### **Risk 1: AI Making Incorrect Review Decisions**\n\n**Probability**: High (AI is not perfect)  \n**Impact**: Medium-High (developers lose trust, real issues missed)\n\n**Mitigation Strategies:**\n\n1. **Human-in-the-Loop for Critical Decisions**\n   - AI provides recommendations, humans make final approval\n   - Auto-block only for critical security issues (high confidence)\n   - All other issues are warnings, not blockers\n\n2. **Confidence Scoring**\n   ```python\n   if ai_review.confidence_score < 0.7:\n       flag_for_human_review()\n   elif ai_review.severity == \"critical\" and ai_review.confidence_score < 0.9:\n       require_security_team_review()\n   ```\n\n3. **Feedback Loop with Monitoring**\n   - Track false positive/negative rates daily\n   - Automatic alerts if false positive rate > 20%\n   - Weekly review of flagged edge cases\n\n4. **Shadow Mode for New Features**\n   - Run new AI features in \"shadow mode\" (log results, don't act on them)\n   - Compare shadow results with human reviews for 2 weeks\n   - Only enable blocking if accuracy > 85%\n\n5. **Override Mechanism**\n   - Developers can override AI decisions with justification\n   - Senior engineers can approve despite AI blocks\n   - Track override patterns to improve AI\n\n---\n\n#### **Risk 2: System Downtime During Critical Deployments**\n\n**Probability**: Medium (systems fail)  \n**Impact**: Critical (blocks urgent hotfixes)\n\n**Mitigation Strategies:**\n\n1. **Bypass Mode for Emergencies**\n   ```yaml\n   # .github/bypass-ai-review\n   reason: \"Production outage - database down\"\n   approved_by: \"senior-engineer@company.com\"\n   incident_ticket: \"INC-12345\"\n   ```\n   - Must include incident ticket\n   - Requires senior engineer approval\n   - Post-incident review mandatory\n\n2. **High Availability Architecture**\n   - Multi-region deployment (primary: us-east-1, failover: us-west-2)\n   - Load balancer with health checks\n   - Database replication and automated failover\n   - **SLA**: 99.9% uptime (< 44 minutes downtime/month)\n\n3. **Graceful Degradation**\n   ```python\n   if ai_service.is_down():\n       # Fall back to basic checks only\n       run_linting()\n       run_security_scan()\n       skip_ai_review()\n       post_comment(\"AI review temporarily unavailable - proceeding with basic checks\")\n   ```\n\n4. **Circuit Breaker Pattern**\n   - If AI service fails 3x in 5 minutes, circuit opens\n   - System falls back to basic review mode\n   - Automatic recovery when service is healthy\n\n5. **Incident Response Plan**\n   - On-call rotation for platform team\n   - Runbooks for common failures\n   - PagerDuty integration for critical alerts\n   - **Response SLA**: Acknowledge in 15min, resolve in 2 hours\n\n---\n\n#### **Risk 3: Integration Failures with Existing Tools**\n\n**Probability**: Medium (complex integrations)  \n**Impact**: Medium (delays rollout)\n\n**Mitigation Strategies:**\n\n1. **Abstraction Layers for External Tools**\n   ```python\n   # If GitHub API changes, only update adapter\n   class GitHubAdapter:\n       def get_pr_diff(self, pr_id):\n           # Wraps GitHub API\n   \n   class GitLabAdapter:\n       def get_pr_diff(self, mr_id):\n           # Wraps GitLab API\n   ```\n\n2. **Comprehensive Integration Testing**\n   - Automated tests against sandbox GitHub/GitLab instances\n   - Contract tests for API integrations\n   - Weekly integration test suite run\n   - **Target**: >90% integration test coverage\n\n3. **Phased Integration**\n   - Start with GitHub only (highest priority)\n   - Add GitLab in Phase 2\n   - Bitbucket in Phase 3 (if needed)\n   - Don't block on perfect multi-platform support\n\n4. **Vendor API Monitoring**\n   - Subscribe to GitHub/GitLab API changelogs\n   - Monitor for deprecated endpoints\n   - Test against beta APIs before they go live\n\n5. **Fallback to Manual Processes**\n   - If integration fails, send review via email\n   - Provide web UI for manual review retrieval\n   - Don't block developers while fixing integration\n\n---\n\n#### **Risk 4: Resistance from Development Teams**\n\n**Probability**: Medium-High (change management is hard)  \n**Impact**: High (adoption fails, project canceled)\n\n**Mitigation Strategies:**\n\n1. **Early Involvement & Co-Design**\n   - Form working group with representatives from each team\n   - Solicit feedback on design decisions\n   - Run bi-weekly demos to show progress\n   - Incorporate team suggestions (e.g., \"Add Python type hint checks\")\n\n2. **Opt-In Pilot Program**\n   - Teams volunteer to be early adopters (not forced)\n   - Provide white-glove support during pilot\n   - Showcase success stories internally\n\n3. **Clear Value Proposition**\n   - Publish metrics: \"Pilot team reduced review time by 60%\"\n   - Show security catches: \"AI detected 15 SQL injection risks\"\n   - Highlight time savings: \"Avg 90 minutes saved per PR\"\n\n4. **Education & Training**\n   - Lunch-and-learn sessions: \"How AI code review works\"\n   - Office hours for questions\n   - Documentation: \"How to interpret AI feedback\"\n   - Video tutorials\n\n5. **Gradual Enforcement**\n   - Month 1-2: AI feedback is optional, informational only\n   - Month 3-4: AI blocks critical security issues only\n   - Month 5-6: Full enforcement (can still override)\n\n6. **Developer Feedback Channel**\n   - Slack channel: #ai-review-feedback\n   - Monthly surveys on satisfaction\n   - Public roadmap showing requested features\n   - Fast response to complaints (<24 hours)\n\n---\n\n#### **Risk 5: Compliance/Audit Requirements**\n\n**Probability**: Medium (regulated industries)  \n**Impact**: High (legal/regulatory issues)\n\n**Mitigation Strategies:**\n\n1. **Audit Trail for All Decisions**\n   ```python\n   AuditLog.record(\n       timestamp=datetime.utcnow(),\n       action=\"pr_approved\",\n       pr_id=\"PR-12345\",\n       approver=\"jane.doe@company.com\",\n       ai_recommendation=\"approve\",\n       human_decision=\"approve\",\n       justification=\"All checks passed\",\n       compliance_frameworks=[\"PCI-DSS\", \"SOC2\"]\n   )\n   ```\n\n2. **Immutable Logs**\n   - Store audit logs in append-only S3 bucket\n   - Object Lock enabled (cannot delete/modify)\n   - Retention: 7 years (compliance requirement)\n\n3. **Explainability for AI Decisions**\n   - AI must cite reasons for each finding\n   - Link to CWE/OWASP standards\n   - Provide remediation steps\n   - No \"black box\" decisions\n\n4. **Human Approval for Compliance-Critical Systems**\n   - Payment systems: Require security team approval\n   - Healthcare systems: Require HIPAA compliance check\n   - Financial systems: Require finance team approval\n\n5. **Regular Compliance Audits**\n   - Quarterly internal audit of AI decisions\n   - Annual external audit (SOC 2)\n   - Penetration testing of AI system itself\n   - Review all overrides (were they justified?)\n\n6. **Compliance Framework Integration**\n   - Built-in PCI-DSS, HIPAA, SOC 2 checks\n   - Auto-generate compliance reports\n   - Alert on compliance violations immediately\n\n---\n\n### Question 4.3: Tool Selection & Integration\n\n#### **Code Review Platforms**\n\n**Primary: GitHub** (highest priority - most teams use it)\n\n**Integration Approach:**\n- **GitHub App** (not webhook) - better permissions, easier auth\n- **Required Permissions**:\n  - Read: Repository content, pull requests, checks\n  - Write: Pull request comments, commit statuses, checks\n- **Webhook Events**: `pull_request` (opened, synchronized), `pull_request_review`\n\n**Secondary: GitLab, Bitbucket** (Phase 2/3)\n- Use adapter pattern to support multiple platforms\n- Unified internal PR representation\n\n**Libraries:**\n```python\n# GitHub\nfrom github import Github\ngithub_client = Github(os.getenv(\"GITHUB_APP_TOKEN\"))\n\n# GitLab\nimport gitlab\ngitlab_client = gitlab.Gitlab(os.getenv(\"GITLAB_URL\"), private_token=os.getenv(\"GITLAB_TOKEN\"))\n```\n\n---\n\n#### **CI/CD Systems**\n\n**Primary: GitHub Actions** (native integration)\n\n**Integration:**\n```yaml\n# .github/workflows/ai-review.yml\nname: AI Code Review\non: [pull_request]\njobs:\n  ai-review:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Run AI Review\n        uses: company/ai-review-action@v1\n        with:\n          pr_number: ${{ github.event.pull_request.number }}\n          api_key: ${{ secrets.AI_REVIEW_API_KEY }}\n```\n\n**Secondary: Jenkins, GitLab CI, CircleCI**\n- Provide generic webhook endpoint for any CI system\n- CI system calls our API at end of build\n\n**Deployment Tools:**\n- **AWS**: boto3 for ECS/Lambda deployments, CDK for infrastructure\n- **Kubernetes**: kubectl, Helm charts\n- **Terraform**: For multi-cloud infrastructure\n\n---\n\n#### **Monitoring Tools**\n\n**Production Metrics: Datadog** (assumed current monitoring)\n\n**Integration:**\n```python\nfrom datadog import initialize, api\n\n# Fetch metrics during post-deployment monitoring\ndef get_deployment_metrics(service_name):\n    query = f\"avg:trace.flask.request.duration{{service:{service_name}}}\"\n    metrics = api.Metric.query(start=deploy_time, end=deploy_time + 3600, query=query)\n    return metrics\n```\n\n**Alternatives: Prometheus, New Relic**\n- Use adapter pattern (same as deployment targets)\n- Normalize metrics format internally\n\n**Internal Metrics (for AI review system itself):**\n- **Prometheus** + **Grafana** for system health dashboards\n- Track: review latency, AI API costs, false positive rate\n- Alerts: If review time >10min or cost >$100/day\n\n---\n\n#### **Security Scanning Tools**\n\n**Static Analysis (SAST):**\n- **Python**: Bandit, Semgrep\n- **JavaScript/TypeScript**: ESLint security plugin, Semgrep\n- **Go**: gosec\n- **Java**: SpotBugs, Semgrep\n\n**Dependency Scanning (SCA):**\n- **Snyk** (commercial, good API, supports all languages)\n- **npm audit** (JavaScript)\n- **pip-audit** (Python)\n- **OWASP Dependency-Check** (Java)\n\n**Integration:**\n```python\n# Snyk API\nimport snyk\nclient = snyk.SnykClient(os.getenv(\"SNYK_TOKEN\"))\nvulns = client.test_python(requirements_file=\"requirements.txt\")\n\n# Bandit\nimport bandit\nfrom bandit.core import manager\nmgr = manager.BanditManager(config_file, \"file\")\nmgr.discover_files([file_path])\nmgr.run_tests()\nresults = mgr.get_issue_list()\n```\n\n**Cost Optimization:**\n- Use open-source tools for basic checks (Bandit, gosec)\n- Use Snyk for comprehensive SCA (pay per scan)\n- Cache results for unchanged dependencies\n\n---\n\n#### **Communication Tools**\n\n**Slack** (primary)\n```python\nfrom slack_sdk import WebClient\nslack = WebClient(token=os.getenv(\"SLACK_BOT_TOKEN\"))\n\nslack.chat_postMessage(\n    channel=\"#dev-team\",\n    text=f\"PR <{pr_url}|#{pr_number}> ready for review\\n\\n:white_check_mark: All checks passed\\n:warning: 2 performance warnings\"\n)\n```\n\n**Microsoft Teams** (if needed)\n```python\nimport pymsteams\nteams_message = pymsteams.connectorcard(os.getenv(\"TEAMS_WEBHOOK_URL\"))\nteams_message.text(\"PR review complete\")\nteams_message.send()\n```\n\n**Jira** (for tracking issues)\n```python\nfrom jira import JIRA\njira = JIRA(server=os.getenv(\"JIRA_URL\"), basic_auth=(user, token))\n\n# Auto-create tickets for critical issues\nif issue.severity == \"critical\":\n    jira.create_issue(\n        project=\"SEC\",\n        summary=f\"Critical security issue in PR {pr_id}\",\n        description=issue.description,\n        issuetype=\"Bug\",\n        priority=\"Highest\"\n    )\n```\n\n---\n\n#### **LLM Platform for AI Review**\n\n**Primary: OpenAI GPT-4** (or GPT-4 Turbo)\n```python\nfrom openai import OpenAI\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4-turbo\",\n    messages=[\n        {\"role\": \"system\", \"content\": code_review_prompt},\n        {\"role\": \"user\", \"content\": pr_diff}\n    ],\n    temperature=0.1\n)\n```\n\n**Alternatives:**\n- **Anthropic Claude** (good for code, longer context)\n- **AWS Bedrock** (Claude via AWS, better for enterprises)\n- **Self-hosted**: CodeLlama, StarCoder (cost-effective at scale)\n\n**Cost Management:**\n- Use GPT-3.5 for simple PRs (< 100 lines changed)\n- Use GPT-4 for complex PRs (> 100 lines or security-sensitive)\n- Cache AI responses for identical code (avoid re-analysis)\n\n**Estimated Costs:**\n- GPT-4: ~$0.10 per review (3k input tokens, 500 output)\n- At 500 PRs/week: ~$50/week = $200/month\n- Total AI cost: < $500/month (well within budget)\n\n---\n\n### **Technology Stack Summary**\n\n| **Category** | **Tool** | **Purpose** |\n|--------------|---------|-------------|\n| **Code Hosting** | GitHub (primary), GitLab (secondary) | Source control, PR management |\n| **CI/CD** | GitHub Actions, Jenkins | Build & test automation |\n| **AI/LLM** | OpenAI GPT-4, AWS Bedrock | Code review intelligence |\n| **Security Scanning** | Snyk, Bandit, Semgrep | Vulnerability detection |\n| **Monitoring** | Datadog, Prometheus, Grafana | Deployment metrics, system health |\n| **Communication** | Slack, Microsoft Teams, Jira | Notifications, issue tracking |\n| **Deployment** | AWS (ECS/Lambda), Kubernetes | Production deployment |\n| **Infrastructure** | Terraform, AWS CDK | Infrastructure as code |\n| **Database** | PostgreSQL (audit logs), Redis (cache) | Data persistence |\n| **Orchestration** | AWS Step Functions, Temporal | Workflow management |\n\n**Total Estimated Cost:** $5,000-8,000/month\n- Cloud infrastructure: $3,000/month\n- Snyk licenses: $1,500/month\n- OpenAI API: $500/month\n- Datadog: $2,000/month\n- Slack: Free (existing)\n\n---\n\n### **Success Dashboard**\n\n**KPIs to Track:**\n\n| **Metric** | **Baseline** | **Target (Month 6)** | **Measurement** |\n|------------|-------------|---------------------|----------------|\n| Review Time | 2-3 days | < 4 hours | Time from PR open to approval |\n| Security Detection | ~60% | ‚â• 90% | Critical vulns caught pre-deploy |\n| Deployment Success Rate | 85% | ‚â• 97% | Deploys without rollback |\n| Developer Satisfaction | N/A | ‚â• 80% | Monthly survey (1-5 scale) |\n| False Positive Rate | N/A | < 15% | Issues marked \"not helpful\" |\n| Cost per Review | N/A | < $2 | Total cost / reviews |\n\n**Weekly Review Meetings:**\n- Review metrics dashboard\n- Discuss edge cases and failures\n- Prioritize improvements\n- Celebrate wins (security catches, time saved)"
  },
  {
   "cell_type": "markdown",
   "id": "584added",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}